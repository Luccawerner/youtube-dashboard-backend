"""
analyzer.py - L√≥gica de an√°lise para Analysis Tab
Author: Claude Code
Date: 2024-11-05

Fun√ß√µes:
- analyze_keywords(): Extrai top 20 keywords dos t√≠tulos
- analyze_title_patterns(): Detecta padr√µes de t√≠tulo vencedores
- analyze_top_channels(): Identifica top 5 canais por subniche
- analyze_gaps(): Compara o que concorrentes fazem vs nossos canais
"""

import re
from collections import Counter
from datetime import datetime, timedelta
from typing import List, Dict, Tuple
import json

# Stop words expandido (portugu√™s + ingl√™s + termos gen√©ricos)
STOP_WORDS = {
    # Ingl√™s b√°sico - artigos, preposi√ß√µes, conjun√ß√µes
    'a', 'an', 'and', 'are', 'as', 'at', 'be', 'by', 'for', 'from', 'has', 'he', 'she',
    'in', 'is', 'it', 'its', 'of', 'on', 'that', 'the', 'to', 'was', 'will', 'with',
    'but', 'when', 'they', 'until', 'who', 'what', 'where', 'why', 'how', 'this',
    'these', 'those', 'then', 'than', 'have', 'had', 'been', 'being', 'do', 'does',
    'did', 'or', 'if', 'can', 'could', 'would', 'should', 'may', 'might', 'not',
    'we', 'us', 'our', 'ours', 'you', 'your', 'yours', 'him', 'his', 'her', 'hers',
    'them', 'their', 'theirs', 'me', 'my', 'mine', 'myself', 'yourself', 'himself',
    'herself', 'itself', 'ourselves', 'themselves',

    # Verbos comuns (n√£o s√£o keywords relevantes)
    'said', 'told', 'asked', 'gave', 'made', 'went', 'got', 'took', 'came', 'put',
    'thought', 'looked', 'wanted', 'used', 'found', 'knew', 'called', 'tried',
    'left', 'felt', 'became', 'seemed', 'turned', 'kept', 'began', 'brought',
    'happened', 'showed', 'heard', 'needed', 'moved', 'lived', 'believed', 'held',
    'saw', 'let', 'met', 'ran', 'paid', 'sat', 'spoke', 'stood', 'understood',

    # Portugu√™s b√°sico
    'o', 'a', 'os', 'as', 'de', 'da', 'do', 'das', 'dos', 'em', 'no', 'na', 'nos',
    'nas', 'por', 'para', 'com', 'sem', 'sob', 'sobre', 'um', 'uma', 'uns', 'umas',
    'e', 'ou', 'mas', 'que', 'se', 'quando', 'como', 'onde', 'porque', 'qual',
    'meu', 'minha', 'meus', 'minhas', 'seu', 'sua', 'seus', 'suas', 'ele', 'ela',
    'eles', 'elas', 'n√≥s', 'v√≥s', 'lhe', 'lhes', 'me', 'te', 'nos', 'vos',

    # Verbos comuns portugu√™s
    'disse', 'falou', 'fez', 'foi', 'era', 'tinha', 'estava', 'fui', 'deu', 'viu',
    'sabia', 'pode', 'podia', 'deve', 'quer', 'quis', 'sabe', 'vai', 'vou', 'tem',
    'ser', 'estar', 'ter', 'fazer', 'dar', 'ver', 'saber', 'poder', 'dever', 'querer',

    # Termos gen√©ricos de v√≠deo
    'music', 'video', 'channel', 'new', 'best', 'top', 'hour', 'hours', 'minute',
    'minutes', 'full', 'hd', 'hq', '1080p', '720p', '4k', '2024', '2023', '2022',
    '2025', 'watch', 'subscribe', 'like', 'comment', 'share', 'playlist', 'official',
    'v√≠deo', 'canal', 'completo', 'novo', 'nova', 'melhor', 'melhores', 'compilation'
}


class Analyzer:
    """Classe principal para an√°lises do dashboard"""

    def __init__(self, db_client):
        """
        Inicializa o analisador

        Args:
            db_client: Cliente Supabase para acesso ao banco
        """
        self.db = db_client

    # =========================================================================
    # AN√ÅLISE DE KEYWORDS
    # =========================================================================

    def analyze_keywords(self, subniche: str = None, period_days: int = 30) -> List[Dict]:
        """
        Analisa keywords mais frequentes nos t√≠tulos dos v√≠deos (v√≠deos com 50k+ views)

        Args:
            subniche: Subniche espec√≠fico para analisar (opcional - se None, analisa todos)
            period_days: Per√≠odo em dias (7, 15 ou 30)

        Returns:
            Lista com top 10 keywords ordenadas por performance (views m√©dias)
        """
        subniche_text = f"subniche {subniche}" if subniche else "todos os subniches"
        print(f"[Analyzer] Analisando keywords ({subniche_text}, √∫ltimos {period_days} dias, 50k+ views)...")

        # Buscar v√≠deos do per√≠odo (publicados nos √∫ltimos X dias com 50k+ views)
        cutoff_date = datetime.now() - timedelta(days=period_days)

        query = self.db.table("videos_historico")\
            .select("video_id, titulo, views_atuais, data_publicacao, canais_monitorados!inner(subnicho)", count="exact")\
            .gte("data_publicacao", cutoff_date.strftime("%Y-%m-%d"))\
            .gte("views_atuais", 50000)\
            .limit(100000)

        # Se subniche foi especificado, filtrar
        if subniche:
            query = query.eq("canais_monitorados.subnicho", subniche)

        response = query.execute()
        videos = response.data

        if not videos:
            print(f"[Analyzer] Nenhum v√≠deo encontrado no per√≠odo com 50k+ views para {subniche_text}")
            return []

        print(f"[Analyzer] {len(videos)} v√≠deos encontrados (50k+ views) em {subniche_text}")

        # Extrair keywords de todos os t√≠tulos
        keyword_stats = {}  # {keyword: {'count': int, 'total_views': int, 'video_ids': set}}

        for video in videos:
            titulo = video.get('titulo', '').lower()
            views = video.get('views_atuais', 0)
            video_id = video.get('video_id', '')

            # Extrair palavras (remover n√∫meros, pontua√ß√£o)
            words = re.findall(r'\b[a-z√†-√∫]{3,}\b', titulo)  # Suporta acentos

            for word in words:
                # Pular stop words
                if word in STOP_WORDS:
                    continue

                if word not in keyword_stats:
                    keyword_stats[word] = {
                        'count': 0,
                        'total_views': 0,
                        'videos': set()
                    }

                keyword_stats[word]['count'] += 1
                keyword_stats[word]['total_views'] += views
                keyword_stats[word]['videos'].add(video_id)

        # Calcular score e ordenar
        keyword_list = []
        for keyword, stats in keyword_stats.items():
            video_count = len(stats['videos'])

            # CORRE√á√ÉO: avg_views = total / n√∫mero de V√çDEOS √∫nicos (n√£o ocorr√™ncias)
            avg_views = stats['total_views'] // video_count if video_count > 0 else 0

            # Score: performance m√©dias ** 0.7 √ó ‚àö(v√≠deos √∫nicos) - prioriza keywords com alta performance
            score = (avg_views ** 0.7) * (video_count ** 0.5)

            keyword_list.append({
                'keyword': keyword,
                'frequency': stats['count'],
                'avg_views': avg_views,
                'video_count': video_count,
                'score': score
            })

        # Ordenar por score e pegar top 10
        keyword_list.sort(key=lambda x: x['score'], reverse=True)
        top_10 = keyword_list[:10]

        print(f"[Analyzer] Top {len(top_10)} keywords identificadas")
        return top_10

    # =========================================================================
    # AN√ÅLISE DE PADR√ïES DE T√çTULO - DETEC√á√ÉO AUTOM√ÅTICA GRATUITA
    # =========================================================================

    # Dicion√°rios de categoriza√ß√£o SEPARADOS por tipo de conte√∫do
    CATEGORIAS_STORYTELLING = {
        'RELA√á√ÉO FAMILIAR': [
            'marido', 'esposa', 'mulher', 'esposo', 'husband', 'wife',
            'filho', 'filha', 'filhos', 'son', 'daughter', 'children',
            'pai', 'm√£e', 'pais', 'father', 'mother', 'parents', 'dad', 'mom',
            'sogro', 'sogra', 'sogros', 'father-in-law', 'mother-in-law',
            'irm√£o', 'irm√£', 'brother', 'sister', 'siblings',
            'tio', 'tia', 'uncle', 'aunt',
            'av√¥', 'av√≥', 'grandfather', 'grandmother', 'grandparents',
            'neto', 'neta', 'grandson', 'granddaughter',
            'primo', 'prima', 'cousin',
            'cunhado', 'cunhada', 'genro', 'nora',
            'padrasto', 'madrasta', 'stepfather', 'stepmother',
            'fam√≠lia', 'parente', 'parentes', 'family', 'relative'
        ],
        'TRAG√âDIA': [
            'faleceu', 'morreu', 'morte', 'died', 'death', 'passed', 'funeral',
            'enterro', 'vel√≥rio', 'burial',
            'acidente', 'trag√©dia', 'accident', 'tragedy',
            'doen√ßa', 'c√¢ncer', 'hospital', 'disease', 'cancer', 'illness',
            'assassinato', 'assassinada', 'murder', 'killed',
            'desapareceu', 'disappeared', 'missing'
        ],
        'HERAN√áA': [
            'herdou', 'heran√ßa', 'herdeira', 'inherited', 'inheritance', 'heir',
            'testamento', 'will', 'testament',
            'bens', 'patrim√¥nio', 'property', 'estate',
            'deixou', 'legado', 'left', 'legacy',
            'fortuna', 'fortune', 'wealth'
        ],
        'INJUSTI√áA': [
            'injusti√ßa', 'injusto', 'injustice', 'unfair',
            'traiu', 'trai√ß√£o', 'betrayed', 'betrayal', 'cheated',
            'abandonou', 'abandoned', 'left',
            'expulsou', 'kicked', 'expelled',
            'roubou', 'roubo', 'stole', 'robbed', 'theft',
            'enganou', 'mentiu', 'mentira', 'lied', 'deceived', 'lie',
            'excluiu', 'excluded',
            'humilhou', 'humilha√ß√£o', 'humiliated',
            'desprezou', 'desprezo', 'ignored', 'rejected',
            'restou', 'sobrou', 'left with'
        ],
        'EMO√á√ÉO FORTE': [
            'furioso', 'furiosa', 'f√∫ria', 'furious', 'angry', 'rage',
            'chocado', 'chocada', 'choque', 'shocked', 'stunned',
            'chorou', 'choro', 'chorando', 'cried', 'crying', 'tears',
            'gritou', 'grito', 'gritando', 'screamed', 'yelled', 'shouted',
            'explodiu', 'explos√£o', 'exploded',
            'revoltado', 'revolta', 'revolted',
            'desesperado', 'desespero', 'desperate',
            'arrasado', 'arrasada', 'devastated'
        ],
        'REVIRAVOLTA': [
            'desapareci', 'disappeared',
            'fugi', 'fuga', 'ran', 'escape',
            'sa√≠', 'left',
            'abandonei', 'vinguei', 'vingan√ßa', 'revenge',
            'revelei', 'revela√ß√£o', 'revealed',
            'descobri', 'descoberta', 'discovered', 'found',
            'confessei', 'confiss√£o', 'confessed',
            'ent√£o', 'then', 'but'
        ],
        'CITA√á√ÉO': [
            'disse', 'falou', 'said', 'told',
            'gritou', 'screamed', 'yelled',
            'perguntou', 'asked',
            'respondeu', 'replied'
        ],
        'SEGREDO': [
            'segredo', 'secret', 'hidden',
            'escondido', 'oculto',
            'verdade', 'truth',
            'mist√©rio', 'mystery'
        ],
        'CONFLITO': [
            'briga', 'fight', 'argument',
            'discuss√£o', 'dispute',
            'guerra', 'war',
            'problema', 'problem'
        ]
    }

    CATEGORIAS_MUSICA = {
        'DURA√á√ÉO': ['hours', 'hour', 'horas', 'hora', 'minutos', 'minutes', 'mins'],
        'G√äNERO MUSICAL': [
            'jazz', 'blues', 'lofi', 'lo-fi', 'rock', 'classical', 'pop',
            'piano', 'guitar', 'saxophone', 'violin', 'instrumental'
        ],
        'CONTEXTO USO': [
            'study', 'sleep', 'relax', 'work', 'focus', 'meditation',
            'estudar', 'dormir', 'relaxar', 'trabalhar', 'focar'
        ],
        'FREQU√äNCIA': ['hz', 'hertz', '432hz', '528hz', '639hz', '741hz'],
        'BENEF√çCIO': [
            'healing', 'meditation', 'peace', 'calm', 'calming',
            'energia', 'cura', 'relaxamento', 'peaceful'
        ],
        'AMBIENTE': [
            'rainy', 'rain', 'snowy', 'winter', 'summer', 'night', 'morning',
            'cozy', 'chill', 'relaxing', 'smooth', 'soft'
        ]
    }

    CATEGORIAS_HISTORIA = {
        'TEMA HIST√ìRICO': [
            'ancient', 'history', 'civilization', 'empire', 'kingdom',
            'war', 'battle', 'medieval', 'renaissance',
            'antiguidade', 'hist√≥ria', 'civiliza√ß√£o', 'imp√©rio', 'reino'
        ],
        'MIST√âRIO': [
            'mystery', 'unsolved', 'secret', 'hidden', 'unknown',
            'mist√©rio', 'inexplic√°vel', 'desconhecido'
        ],
        'DESCOBERTA': [
            'discovered', 'found', 'revealed', 'uncovered',
            'descoberta', 'revela√ß√£o', 'achado'
        ]
    }

    def _get_subniche_type(self, subniche: str) -> str:
        """
        Determina o tipo de conte√∫do baseado no subniche

        Returns:
            'storytelling', 'music' ou 'history'
        """
        storytelling_niches = [
            'Contos Familiares', 'Hist√≥rias Motivacionais', 'Hist√≥rias Sombrias',
            'Hist√≥rias Aleat√≥rias', 'Storytelling', 'Contos'
        ]

        music_niches = [
            'Jazz', 'Blues', 'Lofi', 'Classical', 'Piano', 'Focus',
            'M√∫sica', 'Music', 'Frequencies', 'Frequ√™ncias', 'Medita√ß√£o', 'Meditation'
        ]

        history_niches = [
            'Antiguidade', 'Hist√≥ria Antiga', 'Civiliza√ß√µes', 'History',
            'Ancient', 'Mist√©rios', 'Mysteries'
        ]

        if subniche in storytelling_niches:
            return 'storytelling'
        elif subniche in music_niches:
            return 'music'
        elif subniche in history_niches:
            return 'history'
        else:
            # Default para storytelling (maioria dos nossos canais)
            return 'storytelling'

    def analyze_title_patterns(self, subniche: str, period_days: int = 30) -> List[Dict]:
        """
        Detecta padr√µes de t√≠tulo automaticamente (GRATUITO - sem IA)

        Estrat√©gia:
        1. Busca TODOS os v√≠deos com 50k+ views do subniche (√∫ltimos 30 dias publica√ß√£o)
        2. Analisa estrutura: categoriza palavras, detecta CAPS, dinheiro, cita√ß√µes
        3. Agrupa t√≠tulos similares por caracter√≠sticas
        4. Retorna top 5 padr√µes com estrutura simples: [CAT1] + [CAT2] + [CAT3]

        Args:
            subniche: Subniche a analisar
            period_days: Per√≠odo em dias (7, 15 ou 30)

        Returns:
            Lista com top 5 padr√µes: structure, example_title, avg_views, video_count
        """
        print(f"[Analyzer] Analisando padr√µes de t√≠tulo ({subniche}, √∫ltimos 30 dias, 50k+ views, SEM LIMITE)...")

        # Buscar TODOS os v√≠deos do subniche (√∫ltimos 30 dias publica√ß√£o, 50k+ views)
        cutoff_publication = datetime.now() - timedelta(days=30)

        response = self.db.table("videos_historico")\
            .select("video_id, titulo, views_atuais, data_publicacao, data_coleta, canais_monitorados!inner(subnicho)", count="exact")\
            .eq("canais_monitorados.subnicho", subniche)\
            .gte("data_publicacao", cutoff_publication.strftime("%Y-%m-%d"))\
            .gte("views_atuais", 50000)\
            .order("views_atuais", desc=True)\
            .limit(100000)\
            .execute()

        videos = response.data

        if not videos:
            print(f"[Analyzer] Nenhum v√≠deo 50k+ encontrado para {subniche}")
            return []

        print(f"[Analyzer] {len(videos)} v√≠deos encontrados para an√°lise (TODOS do subniche)")

        # Analisa estrutura de cada t√≠tulo
        analyzed_titles = []
        for video in videos:
            features = self._analyze_title_structure(video['titulo'], subniche)
            features['titulo'] = video['titulo']
            features['views'] = video['views_atuais']
            features['video_id'] = video['video_id']
            analyzed_titles.append(features)

        # Agrupa por estrutura similar e extrai padr√µes
        patterns = self._group_by_pattern(analyzed_titles)

        # Ordena por performance e retorna top 5
        patterns.sort(key=lambda x: x['avg_views'], reverse=True)
        top_5 = patterns[:5]

        print(f"[Analyzer] {len(top_5)} padr√µes identificados para {subniche}")
        return top_5

    def _analyze_title_structure(self, titulo: str, subniche: str) -> Dict:
        """
        Analisa caracter√≠sticas estruturais de um t√≠tulo

        Args:
            titulo: T√≠tulo do v√≠deo
            subniche: Subniche para determinar tipo de conte√∫do

        Returns:
            Dict com features detectadas
        """
        features = {
            'categorias': [],  # Categorias detectadas (FAM√çLIA, TRAG√âDIA, etc)
            'has_money': False,
            'money_value': None,
            'has_caps': False,
            'caps_words': [],
            'has_quote': False,
            'has_suspense': False,
            'has_exclamation': False,
            'word_count': len(titulo.split())
        }

        titulo_lower = titulo.lower()

        # Seleciona dicion√°rio de categorias baseado no tipo de subniche
        subniche_type = self._get_subniche_type(subniche)

        if subniche_type == 'storytelling':
            categories_dict = self.CATEGORIAS_STORYTELLING
        elif subniche_type == 'music':
            categories_dict = self.CATEGORIAS_MUSICA
        else:  # history
            categories_dict = self.CATEGORIAS_HISTORIA

        # Detecta categorias de palavras usando o dicion√°rio correto
        for categoria, palavras in categories_dict.items():
            for palavra in palavras:
                if palavra in titulo_lower:
                    if categoria not in features['categorias']:
                        features['categorias'].append(categoria)
                    break

        # Detecta dinheiro
        money_patterns = [
            r'R\$\s*\d+(?:\.\d+)?\s*(?:milh√µes?|mil)?',
            r'\d+\s*milh√µes?\s*de\s*reais',
            r'\d+\s*mil\s*reais'
        ]
        for pattern in money_patterns:
            match = re.search(pattern, titulo, re.I)
            if match:
                features['has_money'] = True
                features['money_value'] = match.group()
                break

        # Detecta CAPS (palavras em mai√∫sculas)
        caps_words = re.findall(r'\b[A-Z√Å√â√ç√ì√ö√Ç√ä√î√É√ï√á]{3,}\b', titulo)
        if caps_words:
            features['has_caps'] = True
            features['caps_words'] = caps_words

        # Detecta cita√ß√£o (aspas, travess√£o)
        if re.search(r'["\'].*["\']|‚Äî|"|"', titulo):
            features['has_quote'] = True

        # Detecta suspense (retic√™ncias)
        if re.search(r'\.{2,}|‚Ä¶', titulo):
            features['has_suspense'] = True

        # Detecta exclama√ß√£o
        if '!' in titulo:
            features['has_exclamation'] = True

        return features

    def _group_by_pattern(self, analyzed_titles: List[Dict]) -> List[Dict]:
        """Agrupa t√≠tulos por estrutura similar e extrai padr√µes"""

        patterns = []

        # Agrupa por combina√ß√µes de categorias
        category_groups = {}

        for item in analyzed_titles:
            # Cria chave baseada nas categorias (ordenadas)
            cats = sorted(item['categorias'])
            if not cats:
                continue

            key = tuple(cats)

            if key not in category_groups:
                category_groups[key] = []

            category_groups[key].append(item)

        # Para cada grupo, cria um padr√£o
        for categories_tuple, items in category_groups.items():
            if len(items) < 2:  # M√≠nimo 2 v√≠deos para ser considerado padr√£o
                continue

            # Calcula m√©tricas
            avg_views = sum(item['views'] for item in items) // len(items)
            best_item = max(items, key=lambda x: x['views'])

            # Monta estrutura do padr√£o
            structure_parts = list(categories_tuple)

            # Adiciona caracter√≠sticas especiais se presentes na maioria
            money_count = sum(1 for i in items if i['has_money'])
            caps_count = sum(1 for i in items if i['has_caps'])
            quote_count = sum(1 for i in items if i['has_quote'])
            suspense_count = sum(1 for i in items if i['has_suspense'])

            total = len(items)

            if money_count / total >= 0.6:  # 60%+ tem dinheiro
                if 'HERAN√áA' not in structure_parts:
                    structure_parts.append('VALOR FINANCEIRO')

            if quote_count / total >= 0.6:  # 60%+ tem cita√ß√£o
                if 'CITA√á√ÉO' not in structure_parts:
                    structure_parts.insert(min(2, len(structure_parts)), 'CITA√á√ÉO DIRETA')

            if suspense_count / total >= 0.6:  # 60%+ tem suspense
                structure_parts.append('SUSPENSE')

            # Monta estrutura final
            structure = ' + '.join([f'[{part}]' for part in structure_parts])

            patterns.append({
                'pattern_structure': structure,
                'example_title': best_item['titulo'],
                'avg_views': avg_views,
                'video_count': len(items)
            })

        return patterns

    # =========================================================================
    # AN√ÅLISE DE TOP CHANNELS
    # =========================================================================

    def analyze_top_channels(self, subniche: str, period_days: int = 30) -> List[Dict]:
        """
        Identifica top 5 canais por subniche

        Args:
            subniche: Subniche a analisar
            period_days: Per√≠odo em dias (7, 15 ou 30)

        Returns:
            Lista com top 5 canais ordenados por views do per√≠odo
        """
        print(f"[Analyzer] Analisando top channels ({subniche}, {period_days} dias)...")

        # Buscar canais minerados do subniche
        cutoff_date = datetime.now() - timedelta(days=period_days)

        response = self.db.table("dados_canais_historico")\
            .select("*, canais_monitorados!inner(id, nome_canal, url_canal, subnicho, tipo)")\
            .eq("canais_monitorados.subnicho", subniche)\
            .eq("canais_monitorados.tipo", "minerado")\
            .gte("data_coleta", cutoff_date.strftime("%Y-%m-%d"))\
            .order("data_coleta", desc=True)\
            .execute()

        channels_data = response.data

        if not channels_data:
            print(f"[Analyzer] Nenhum canal encontrado para {subniche}")
            return []

        # Mapear per√≠odo para campo de views correto
        views_field_map = {7: 'views_7d', 15: 'views_15d', 30: 'views_30d'}
        views_field = views_field_map.get(period_days, 'views_30d')

        # Agrupar por canal e pegar dados mais recentes
        channels_by_id = {}
        for row in channels_data:
            canal_id = row['canal_id']

            if canal_id not in channels_by_id:
                channels_by_id[canal_id] = {
                    'canal_id': canal_id,
                    'nome_canal': row['canais_monitorados']['nome_canal'],
                    'url_canal': row['canais_monitorados']['url_canal'],
                    'views_period': row.get(views_field, row.get('views_30d', 0)),
                    'views_30d': row.get('views_30d', 0),  # Manter para compatibilidade
                    'inscritos': row['inscritos'],
                    'data_coleta': row['data_coleta']
                }

        # Calcular inscritos ganhos (m√™s atual e m√™s anterior)
        for canal_id, data in channels_by_id.items():
            current_subs = data['inscritos']

            # Buscar snapshot de 30 dias atr√°s (in√≠cio do m√™s atual)
            date_30d_ago = datetime.now() - timedelta(days=30)
            response_30d = self.db.table("dados_canais_historico")\
                .select("inscritos")\
                .eq("canal_id", canal_id)\
                .lte("data_coleta", date_30d_ago.strftime("%Y-%m-%d"))\
                .order("data_coleta", desc=True)\
                .limit(1)\
                .execute()

            subs_30d_ago = response_30d.data[0]['inscritos'] if response_30d.data else current_subs
            data['subscribers_gained_30d'] = current_subs - subs_30d_ago

            # Buscar snapshot de 60 dias atr√°s (in√≠cio do m√™s anterior)
            date_60d_ago = datetime.now() - timedelta(days=60)
            response_60d = self.db.table("dados_canais_historico")\
                .select("inscritos")\
                .eq("canal_id", canal_id)\
                .lte("data_coleta", date_60d_ago.strftime("%Y-%m-%d"))\
                .order("data_coleta", desc=True)\
                .limit(1)\
                .execute()

            subs_60d_ago = response_60d.data[0]['inscritos'] if response_60d.data else subs_30d_ago

            # Inscritos ganhos no m√™s anterior (60-30 dias atr√°s)
            data['subscribers_previous_month'] = subs_30d_ago - subs_60d_ago

            # Crescimento percentual (m√™s atual vs m√™s anterior)
            if data['subscribers_previous_month'] > 0:
                growth = ((data['subscribers_gained_30d'] - data['subscribers_previous_month']) / data['subscribers_previous_month']) * 100
                data['growth_percentage'] = round(growth, 1)
            else:
                data['growth_percentage'] = 0.0

        # Ordenar por views do per√≠odo e pegar top 5
        channel_list = list(channels_by_id.values())
        channel_list.sort(key=lambda x: x['views_period'], reverse=True)
        top_5 = channel_list[:5]

        print(f"[Analyzer] {len(top_5)} canais identificados para {subniche} ({period_days} dias)")
        return top_5

    # =========================================================================
    # GAP ANALYSIS
    # =========================================================================

    def analyze_gaps(self, subniche: str) -> List[Dict]:
        """
        Identifica gaps ESTRAT√âGICOS (o que concorrentes fazem que n√≥s n√£o fazemos)

        Analisa 4 dimens√µes:
        1. Dura√ß√£o dos v√≠deos
        2. Frequ√™ncia de upload
        3. Taxa de engagement (likes/views)
        4. Padr√µes de t√≠tulo (menos priorit√°rio)

        Args:
            subniche: Subniche a analisar

        Returns:
            Lista com no m√°ximo 2 gaps mais importantes
        """
        print(f"[Analyzer] Analisando gaps ESTRAT√âGICOS ({subniche})...")

        gaps = []
        cutoff_date = (datetime.now() - timedelta(days=30)).strftime("%Y-%m-%d")

        # =====================================================================
        # GAP 1: DURA√á√ÉO DOS V√çDEOS
        # =====================================================================
        print(f"[Analyzer] Analisando gap de dura√ß√£o...")

        # Nossos v√≠deos (10k+ views, √∫ltimos 30 dias)
        response_nossos = self.db.table("videos_historico")\
            .select("duracao, canais_monitorados!inner(tipo, subnicho)", count="exact")\
            .eq("canais_monitorados.tipo", "nosso")\
            .eq("canais_monitorados.subnicho", subniche)\
            .gte("data_publicacao", cutoff_date)\
            .gte("views_atuais", 10000)\
            .limit(100000)\
            .execute()

        nossos_videos = response_nossos.data

        # Concorrentes (10k+ views, √∫ltimos 30 dias)
        response_concorrentes = self.db.table("videos_historico")\
            .select("duracao, canais_monitorados!inner(tipo, subnicho)", count="exact")\
            .eq("canais_monitorados.tipo", "minerado")\
            .eq("canais_monitorados.subnicho", subniche)\
            .gte("data_publicacao", cutoff_date)\
            .gte("views_atuais", 10000)\
            .limit(100000)\
            .execute()

        concorrentes_videos = response_concorrentes.data

        if nossos_videos and concorrentes_videos:
            # Calcula dura√ß√£o m√©dia (em segundos)
            nossos_durations = [v.get('duracao', 0) for v in nossos_videos if v.get('duracao', 0) > 0]
            concorrentes_durations = [v.get('duracao', 0) for v in concorrentes_videos if v.get('duracao', 0) > 0]

            if nossos_durations and concorrentes_durations:
                nossa_avg_duration = sum(nossos_durations) / len(nossos_durations)
                concorrente_avg_duration = sum(concorrentes_durations) / len(concorrentes_durations)

                # Diferen√ßa significativa (> 3min = 180s)
                diff_seconds = abs(nossa_avg_duration - concorrente_avg_duration)
                if diff_seconds > 180:
                    diff_percent = ((concorrente_avg_duration / nossa_avg_duration - 1) * 100) if nossa_avg_duration > 0 else 0

                    gaps.append({
                        'type': 'duration',
                        'priority': 'high' if diff_seconds > 600 else 'medium',  # 10min = cr√≠tico
                        'title': 'Dura√ß√£o dos V√≠deos',
                        'your_value': f'{int(nossa_avg_duration // 60)}min',
                        'your_context': 'm√©dia atual',
                        'competitor_value': f'{int(concorrente_avg_duration // 60)}min',
                        'competitor_context': 'top performers',
                        'difference': diff_percent,
                        'impact_description': f"+{abs(diff_percent):.0f}% {'mais longo' if concorrente_avg_duration > nossa_avg_duration else 'mais curto'} = potencial +40-60% views",
                        'actions': [
                            f"Aumentar dura√ß√£o para {int(concorrente_avg_duration // 60)}-{int(concorrente_avg_duration // 60) + 5}min" if concorrente_avg_duration > nossa_avg_duration else f"Reduzir dura√ß√£o para {int(concorrente_avg_duration // 60)}-{int(concorrente_avg_duration // 60) + 5}min",
                            "Analisar v√≠deos top do subniche: quanto tempo duram?",
                            "Manter qualidade - n√£o encher de conte√∫do fraco"
                        ],
                        'priority_text': 'ALTA' if diff_seconds > 1200 else 'M√âDIA',
                        'effort': 'M√©dio',
                        'roi': '+40-60% views estimado'
                    })

        # =====================================================================
        # GAP 2: FREQU√äNCIA DE UPLOAD
        # =====================================================================
        print(f"[Analyzer] Analisando gap de frequ√™ncia...")

        # Contar v√≠deos por canal (nossos)
        response_nossos_canais = self.db.table("videos_historico")\
            .select("canal_id, canais_monitorados!inner(tipo, subnicho)", count="exact")\
            .eq("canais_monitorados.tipo", "nosso")\
            .eq("canais_monitorados.subnicho", subniche)\
            .gte("data_publicacao", cutoff_date)\
            .limit(100000)\
            .execute()

        nossos_canais_videos = response_nossos_canais.data

        # Contar v√≠deos por canal (concorrentes)
        response_concorrentes_canais = self.db.table("videos_historico")\
            .select("canal_id, canais_monitorados!inner(tipo, subnicho)", count="exact")\
            .eq("canais_monitorados.tipo", "minerado")\
            .eq("canais_monitorados.subnicho", subniche)\
            .gte("data_publicacao", cutoff_date)\
            .limit(100000)\
            .execute()

        concorrentes_canais_videos = response_concorrentes_canais.data

        if nossos_canais_videos and concorrentes_canais_videos:
            # Calcular v√≠deos/canal
            nossos_canais_unique = set([v['canal_id'] for v in nossos_canais_videos])
            concorrentes_canais_unique = set([v['canal_id'] for v in concorrentes_canais_videos])

            nossa_freq = len(nossos_canais_videos) / len(nossos_canais_unique) if nossos_canais_unique else 0
            concorrente_freq = len(concorrentes_canais_videos) / len(concorrentes_canais_unique) if concorrentes_canais_unique else 0

            # Diferen√ßa significativa (> 1 v√≠deo/m√™s)
            diff_videos = abs(nossa_freq - concorrente_freq)
            if diff_videos > 1:
                diff_percent = ((concorrente_freq / nossa_freq - 1) * 100) if nossa_freq > 0 else 0

                gaps.append({
                    'type': 'frequency',
                    'priority': 'high' if diff_videos > 4 else 'medium',
                    'title': 'Frequ√™ncia de Upload',
                    'your_value': f'{nossa_freq:.1f} v√≠deos/m√™s',
                    'your_context': 'inconsistente' if nossa_freq < 4 else 'atual',
                    'competitor_value': f'{concorrente_freq:.1f} v√≠deos/m√™s',
                    'competitor_context': 'consistente',
                    'difference': diff_percent,
                    'impact_description': f"+{abs(diff_percent):.0f}% mais conte√∫do = algoritmo favorece +80-120% crescimento",
                    'actions': [
                        f"Passar de {nossa_freq:.1f} para {concorrente_freq:.1f} v√≠deos/m√™s" if concorrente_freq > nossa_freq else f"Reduzir frequ√™ncia e focar em qualidade",
                        "Automatizar produ√ß√£o ou contratar editor adicional" if concorrente_freq > nossa_freq else "Analisar quais v√≠deos performam melhor",
                        "Manter consist√™ncia - postar em dias fixos"
                    ],
                    'priority_text': 'ALTA' if diff_videos > 4 else 'M√âDIA',
                    'effort': 'Alto' if concorrente_freq > nossa_freq else 'Baixo',
                    'roi': '+80-120% crescimento estimado' if concorrente_freq > nossa_freq else '+20-40% qualidade'
                })

        # =====================================================================
        # GAP 3: TAXA DE ENGAGEMENT (DESABILITADO - campo likes_atuais n√£o existe)
        # =====================================================================
        # TODO: Implementar quando campo likes estiver dispon√≠vel na tabela
        print(f"[Analyzer] Gap de engagement desabilitado (campo likes_atuais n√£o dispon√≠vel)")

        # Ordenar por prioridade e retornar no m√°ximo 2 gaps
        gaps.sort(key=lambda x: {'high': 0, 'medium': 1}.get(x['priority'], 2))

        print(f"[Analyzer] {len(gaps[:2])} gaps estrat√©gicos identificados para {subniche}")
        return gaps[:2]  # M√ÅXIMO 2 GAPS MAIS IMPORTANTES

    # =========================================================================
    # SUBNICHE TRENDS ANALYSIS
    # =========================================================================

    def analyze_subniche_trends(self, period_days: int) -> List[Dict]:
        """
        Analisa tend√™ncias por subniche (para pr√©-c√°lculo di√°rio).

        Calcula para cada subniche:
        - Total de v√≠deos publicados no per√≠odo
        - Views m√©dias dos v√≠deos
        - Engagement rate: (likes + comments) / views
        - Tend√™ncia: % crescimento comparado ao per√≠odo anterior

        Args:
            period_days: Per√≠odo em dias (7, 15 ou 30)

        Returns:
            Lista com dados de tend√™ncias de todos os subnichos
        """
        print(f"[Analyzer] Analisando tend√™ncias por subniche ({period_days} dias)...")

        # Buscar todos os subnichos ativos
        response_subnichos = self.db.table("canais_monitorados")\
            .select("subnicho")\
            .eq("status", "ativo")\
            .eq("tipo", "minerado")\
            .execute()

        if not response_subnichos.data:
            print("[Analyzer] Nenhum subniche encontrado")
            return []

        # Extrair subnichos √∫nicos
        subnichos = list(set([item['subnicho'] for item in response_subnichos.data if item.get('subnicho')]))
        print(f"[Analyzer] Processando {len(subnichos)} subnichos...")

        trends = []
        today = datetime.now()
        cutoff_date_current = (today - timedelta(days=period_days)).strftime("%Y-%m-%d")
        cutoff_date_previous = (today - timedelta(days=period_days * 2)).strftime("%Y-%m-%d")

        for subniche in subnichos:
            try:
                # ===============================================================
                # PER√çODO ATUAL (√∫ltimos X dias)
                # ===============================================================

                # Buscar v√≠deos do per√≠odo atual (SEM LIMIT - contagem exata)
                response_current = self.db.table("videos_historico")\
                    .select("video_id, views_atuais, likes, comentarios, canais_monitorados!inner(subnicho, tipo)", count="exact")\
                    .eq("canais_monitorados.subnicho", subniche)\
                    .eq("canais_monitorados.tipo", "minerado")\
                    .gte("data_publicacao", cutoff_date_current)\
                    .limit(100000)\
                    .execute()

                videos_current = response_current.data

                if not videos_current:
                    # Subniche sem v√≠deos no per√≠odo
                    trends.append({
                        'subnicho': subniche,
                        'period_days': period_days,
                        'total_videos': 0,
                        'avg_views': 0,
                        'engagement_rate': 0.0,
                        'trend_percent': 0.0
                    })
                    continue

                # Calcular m√©tricas do per√≠odo atual
                total_videos_current = len(videos_current)
                total_views_current = sum([v.get('views_atuais', 0) for v in videos_current])
                avg_views_current = total_views_current // total_videos_current if total_videos_current > 0 else 0

                # Calcular engagement rate
                total_engagement = 0
                for v in videos_current:
                    likes = v.get('likes', 0) or 0
                    comments = v.get('comentarios', 0) or 0
                    views = v.get('views_atuais', 0) or 0
                    if views > 0:
                        total_engagement += (likes + comments) / views

                engagement_rate = (total_engagement / total_videos_current * 100) if total_videos_current > 0 else 0.0

                # ===============================================================
                # PER√çODO ANTERIOR (para calcular tend√™ncia)
                # ===============================================================

                response_previous = self.db.table("videos_historico")\
                    .select("video_id, views_atuais, canais_monitorados!inner(subnicho, tipo)", count="exact")\
                    .eq("canais_monitorados.subnicho", subniche)\
                    .eq("canais_monitorados.tipo", "minerado")\
                    .gte("data_publicacao", cutoff_date_previous)\
                    .lt("data_publicacao", cutoff_date_current)\
                    .limit(100000)\
                    .execute()

                videos_previous = response_previous.data

                # Calcular views m√©dias do per√≠odo anterior
                if videos_previous:
                    total_videos_previous = len(videos_previous)
                    total_views_previous = sum([v.get('views_atuais', 0) for v in videos_previous])
                    avg_views_previous = total_views_previous // total_videos_previous if total_videos_previous > 0 else 0
                else:
                    avg_views_previous = 0

                # Calcular tend√™ncia (% crescimento)
                if avg_views_previous > 0:
                    trend_percent = ((avg_views_current - avg_views_previous) / avg_views_previous) * 100
                else:
                    trend_percent = 0.0

                # Adicionar resultado
                trends.append({
                    'subnicho': subniche,
                    'period_days': period_days,
                    'total_videos': total_videos_current,
                    'avg_views': avg_views_current,
                    'engagement_rate': round(engagement_rate, 2),
                    'trend_percent': round(trend_percent, 1)
                })

            except Exception as e:
                print(f"[Analyzer] Erro ao processar subniche {subniche}: {e}")
                # Adicionar com valores zero em caso de erro
                trends.append({
                    'subnicho': subniche,
                    'period_days': period_days,
                    'total_videos': 0,
                    'avg_views': 0,
                    'engagement_rate': 0.0,
                    'trend_percent': 0.0
                })

        print(f"[Analyzer] {len(trends)} tend√™ncias calculadas para {period_days} dias")
        return trends


# =========================================================================
# FUN√á√ïES AUXILIARES
# =========================================================================

def save_analysis_to_db(db_client, analysis_type: str, data: List[Dict], period_days: int = None, subniche: str = None):
    """
    Salva resultados da an√°lise no banco

    Args:
        db_client: Cliente Supabase
        analysis_type: Tipo de an√°lise ('keywords', 'patterns', 'channels', 'gaps')
        data: Dados a salvar
        period_days: Per√≠odo analisado (para keywords/patterns)
        subniche: Subniche analisado (para patterns/channels/gaps)
    """
    today = datetime.now().strftime("%Y-%m-%d")

    if analysis_type == 'keywords':
        # Salvar em keyword_analysis
        for item in data:
            record = {
                'keyword': item['keyword'],
                'period_days': period_days,
                'frequency': item['frequency'],
                'avg_views': item['avg_views'],
                'video_count': item['video_count'],
                'analyzed_date': today
            }

            # Adicionar subniche se foi fornecido
            if subniche:
                record['subniche'] = subniche
                conflict_fields = 'keyword,subniche,period_days,analyzed_date'
            else:
                conflict_fields = 'keyword,period_days,analyzed_date'

            db_client.table("keyword_analysis").upsert(record, on_conflict=conflict_fields).execute()

    elif analysis_type == 'patterns':
        # Salvar em title_patterns
        for item in data:
            # Primeiro, deletar registros antigos para esse subniche/period/date
            db_client.table("title_patterns").delete()\
                .eq('subniche', subniche)\
                .eq('period_days', period_days)\
                .eq('analyzed_date', today)\
                .execute()

            # Inserir novo registro
            db_client.table("title_patterns").insert({
                'subniche': subniche,
                'period_days': period_days,
                'pattern_structure': item['pattern_structure'],
                'pattern_description': '',  # Campo vazio (n√£o usado mais)
                'example_title': item['example_title'],
                'avg_views': item['avg_views'],
                'video_count': item['video_count'],
                'analyzed_date': today
            }).execute()

    elif analysis_type == 'channels':
        # Salvar em top_channels_snapshot
        for rank, item in enumerate(data, 1):
            db_client.table("top_channels_snapshot").upsert({
                'canal_id': item['canal_id'],
                'subniche': subniche,
                'views_30d': item['views_30d'],
                'subscribers_gained_30d': item['subscribers_gained_30d'],
                'rank_position': rank,
                'snapshot_date': today
            }, on_conflict='canal_id,subniche,snapshot_date').execute()

    elif analysis_type == 'gaps':
        # Salvar em gap_analysis (NOVA ESTRUTURA)
        week_start = (datetime.now() - timedelta(days=datetime.now().weekday())).strftime("%Y-%m-%d")
        week_end = (datetime.now() + timedelta(days=6-datetime.now().weekday())).strftime("%Y-%m-%d")

        for item in data:
            # Converter nova estrutura para formato da tabela (compatibilidade)
            db_client.table("gap_analysis").upsert({
                'subniche': subniche,
                'gap_title': item['title'],  # Nova estrutura usa 'title'
                'gap_description': item['impact_description'],  # Nova estrutura usa 'impact_description'
                'competitor_count': 0,  # N√£o usado mais
                'avg_views': 0,  # N√£o aplic√°vel para gaps de dura√ß√£o/frequ√™ncia
                'example_videos': json.dumps([]),  # Vazio
                'recommendation': '\n'.join(item['actions']),  # Nova estrutura usa lista 'actions'
                'analyzed_week_start': week_start,
                'analyzed_week_end': week_end
            }, on_conflict='subniche,gap_title,analyzed_week_start').execute()

    elif analysis_type == 'subniche_trends':
        # Salvar em subniche_trends_snapshot
        today = datetime.now().strftime("%Y-%m-%d")

        records = []
        for item in data:
            records.append({
                'subnicho': item['subnicho'],
                'period_days': item['period_days'],
                'total_videos': item['total_videos'],
                'avg_views': item['avg_views'],
                'engagement_rate': item['engagement_rate'],
                'trend_percent': item['trend_percent'],
                'snapshot_date': today,
                'analyzed_date': today
            })

        if records:
            # Upsert: atualiza se existe, cria se n√£o existe (baseado em UNIQUE constraint)
            try:
                response = db_client.table("subniche_trends_snapshot").upsert(
                    records,
                    on_conflict='subnicho,period_days,analyzed_date'
                ).execute()
                print(f"[Analyzer] ‚úÖ {len(records)} tend√™ncias de subnichos salvas ({period_days} dias)")
                print(f"[Analyzer] üìä Response: {len(response.data)} registros retornados")
            except Exception as e:
                print(f"[Analyzer] ‚ùå ERRO ao salvar trends: {e}")
                raise

    print(f"[Analyzer] {len(data)} registros salvos ({analysis_type})")
